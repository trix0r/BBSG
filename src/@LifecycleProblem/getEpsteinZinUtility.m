function [Info, J, gradientJ] = getEpsteinZinUtility(...
    self, ...
    previousSolution, ...
    policy, ...
    state, ...
    discreteState, ...
    t)

calculateGradients = (nargout > 2);

C = self.computeConsumptionForPolicy(state, discreteState, t, policy);

% get index of discrete state
ds = strcmp(self.discreteStateNames, discreteState);

g   = self.riskAversion;
p   = self.elasticityOfIntertemporalSubstitution;
df  = self.discountFactor^(self.Time.ageStep);
b   = self.narrowFramingStrength;

[shocks, probability] = self.computeShockDistribution(state, ...
                                                      discreteState, t);

valueFunctionExpectation = 0;
lossAversionExpectation  = 0;
if calculateGradients
    gradientExpectation = zeros(1, self.numberOfPolicies);
end

Info.numberOfExtrapolations = 0;
Info.Calls.Evaluate.J = 0;
Info.Calls.Evaluate.gradJ = 0;

% for every possible new discrete state
for q = 1:self.numberOfDiscreteStates
    discreteStateName = self.discreteStateNames{q};

    newState = self.computeStateTransition(...
                            state, discreteStateName, t, policy, shocks);
    transitionProbability = self.transitionMatrix(t, ds, q);

    % interpolation
    interpOptJ = previousSolution.interpOptJ.(discreteStateName);
    time_evaluate = tic();
    if calculateGradients
        [interpJ, interpGradientJ] = interpOptJ.evaluate(newState);
        Info.Calls.Evaluate.gradJ = Info.Calls.Evaluate.gradJ + size(newState, 1);
    else
        interpJ = interpOptJ.evaluate(newState);
        Info.Calls.Evaluate.J = Info.Calls.Evaluate.J + size(newState, 1);
    end
    Info.Time.evaluate = toc(time_evaluate);
    Info.numberOfExtrapolations = Info.numberOfExtrapolations + ...
        sum(any(bsxfun(@lt, newState, interpOptJ.lowerBounds) | ...
                bsxfun(@gt, newState, interpOptJ.upperBounds), 2));

    %% calculation of value function expectation
    eValue = transitionProbability * ...
            abs(probability.(discreteStateName)' * ...
                self.computeValueFunctionForPolicy(...
                               interpJ, state, discreteStateName, t, ...
                               policy, shocks).^g);
    valueFunctionExpectation = valueFunctionExpectation + eValue;

    % calculation of loss aversion expectation
    eLoss = transitionProbability * ...
                probability.(discreteStateName)' * ...
                self.computeLossAversion(state, discreteStateName, t, ...
                                 policy, shocks);
    %TODO this depends on the current state, not the new state, but this is
    %specific to the transaction costs problem. How to fix this?
    lossAversionExpectation = lossAversionExpectation + eLoss;

    if calculateGradients
        ge = transitionProbability * ...
                probability.(discreteStateName)' * ...
                (g * bsxfun(@times, self.computeValueFunctionForPolicy(...
                               interpJ, state, discreteStateName, t, ...
                               policy, shocks).^(g - 1), ...
                            self.computeGradientForPolicy(...
                                interpJ, ...
                                interpGradientJ, ...
                                state, discreteStateName, t, ...
                                policy, ...
                                shocks)));
        gradientExpectation = gradientExpectation + ge;
    end  
end

% We follow the preference specfication in Cordoba and Ripoll (2017), RES, no. 84 pp. 1472â€“1509
% Setting p = g gives the certainty equivalent formulation
j = C^p + df * (valueFunctionExpectation^(1/g) + lossAversionExpectation * b(:))^p;
J = j^(1/p);

if calculateGradients
    gradientJ = ...
        1 / p * j^(1 / p - 1) * (p * C^(p - 1) * ...
                self.computeGradientForConsumption(state, discreteState, ...
                                                   t, policy) + ...
                df * gradientExpectation * p/g * ...
                valueFunctionExpectation^(p/g - 1));
end

end
